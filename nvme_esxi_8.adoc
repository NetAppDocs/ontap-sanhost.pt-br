---
sidebar: sidebar 
permalink: nvme_esxi_8.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: 'Você pode configurar o NVMe sobre Fabrics (NVMe-of) em hosts iniciadores que executam o ESXi 8.x e o ONTAP como destino.' 
---
= Configuração de host NVMe-of para ESXi 8.x com ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Você pode configurar o NVMe sobre Fabrics (NVMe-of) em hosts iniciadores que executam o ESXi 8.x e o ONTAP como destino.



== Capacidade de suporte

* A partir da alocação de espaço ONTAP 9.16,1 é ativada por padrão para todos os namespaces NVMe recém-criados.
* A partir do ONTAP 9.9,1 P3, o protocolo NVMe/FC é compatível com ESXi 8 e posterior.
* A partir do ONTAP 9.10,1, o protocolo NVMe/TCP é compatível com ONTAP.




== Caraterísticas

* Os hosts iniciadores do ESXi podem executar o tráfego NVMe/FC e FCP nas mesmas portas do adaptador. Consulte o link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] para obter uma lista de controladores e adaptadores FC compatíveis. Consulte a link:https://mysupport.netapp.com/matrix/["Ferramenta de Matriz de interoperabilidade do NetApp"^] para obter a lista mais atual de configurações e versões suportadas.
* Para ESXi 8,0 e versões posteriores, o HPP (plugin de alto desempenho) é o plug-in padrão para dispositivos NVMe.




== Limitações conhecidas

* O mapeamento RDM não é suportado.




== Habilite o NVMe/FC

O NVMe/FC está habilitado por padrão nas versões do vSphere.

.Verifique o NQN do host
Você deve verificar a string NQN do host ESXi e verificar se ela corresponde à string NQN do host para o subsistema correspondente na matriz ONTAP.

[listing]
----
# esxcli nvme info get
----
Exemplo de saída:

[listing]
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
[listing]
----
# vserver nvme subsystem host show -vserver nvme_fc
----
Exemplo de saída:

[listing]
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
Se as strings NQN do host não corresponderem, você deve usar o `vserver nvme subsystem host add` comando para atualizar a string NQN do host correta no subsistema NVMe do ONTAP correspondente.



== Configure Broadcom/Emulex e Marvell/Qlogic

 `lpfc`O driver e `qlnativefc` o driver do vSphere 8.x têm a capacidade NVMe/FC habilitada por padrão.

link:https://mysupport.netapp.com/matrix/["Ferramenta de Matriz de interoperabilidade"^]Consulte para verificar se a configuração é suportada com o controlador ou firmware.



== Validar o NVMe/FC

Use o procedimento a seguir para validar o NVMe/FC.

.Passos
. Verifique se o adaptador NVMe/FC está listado no host ESXi:
+
[listing]
----
# esxcli nvme adapter list
----
+
Exemplo de saída:

+
[listing]
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----
. Verifique se os namespaces NVMe/FC foram criados corretamente:
+
Os UUIDs no exemplo a seguir representam os dispositivos de namespace NVMe/FC.

+
[listing, subs="+quotes"]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE]
====
No ONTAP 9.7, o tamanho do bloco padrão para um namespace NVMe/FC é 4K. Este tamanho padrão não é compatível com ESXi. Portanto, ao criar namespaces para ESXi, você deve definir o tamanho do bloco de namespace como *512B*. Você pode fazer isso usando o `vserver nvme namespace create` comando.

Exemplo,

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Consulte a link:https://docs.netapp.com/us-en/ontap/concepts/manual-pages.html["Páginas de manual do comando ONTAP 9"^]para obter mais detalhes.

====
. Verifique o status dos caminhos ANA individuais dos respetivos dispositivos de namespace NVMe/FC:
+
[listing, subs="+quotes"]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----




== Configurar o NVMe/TCP

No ESXi 8.x, os módulos NVMe/TCP necessários são carregados por padrão. Para configurar a rede e o adaptador NVMe/TCP, consulte a documentação do VMware vSphere.



== Valide o NVMe/TCP

Você pode usar o procedimento a seguir para validar o NVMe/TCP.

.Passos
. Verifique o status do adaptador NVMe/TCP:
+
[listing]
----
esxcli nvme adapter list
----
+
Exemplo de saída:

+
[listing]
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----
. Recuperar uma lista de conexões NVMe/TCP:
+
[listing]
----
esxcli nvme controller list
----
+
Exemplo de saída:

+
[listing]
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----
. Recuperar uma lista do número de caminhos para um namespace NVMe:
+
[listing, subs="+quotes"]
----
esxcli storage hpp path list -d *uuid.f4f14337c3ad4a639edf0e21de8b88bf*
----
+
Exemplo de saída:

+
[listing, subs="+quotes"]
----
tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.165:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T0:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.168:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T3:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.166:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T2:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.167:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T1:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}
----




== NVMe desalocar

O comando NVMe desalocar é compatível com ESXi 8.0u2 e posterior com o ONTAP 9.16.1 e posterior.

O suporte a desalocar está sempre habilitado para namespaces NVMe. Desalocar também permite que o SO convidado execute operações de 'UNMAP' (às vezes chamadas de 'TRIM') em datastores VMFS. As operações de desalocar permitem que um host identifique blocos de dados que não são mais necessários porque eles não contêm mais dados válidos. O sistema de storage pode remover esses blocos de dados para que o espaço possa ser consumido em outros lugares.

.Passos
. No seu host ESXi, verifique a configuração para DSM desalocar com suporte a TP4040:
+
`esxcfg-advcfg -g /Scsi/NVmeUseDsmTp4040`

+
O valor esperado é 0.

. Ative a definição para DSM desalocar com suporte a TP4040:
+
`esxcfg-advcfg -s 1 /Scsi/NvmeUseDsmTp4040`

. Verifique se a definição para DSM desalocar com suporte a TP4040 está ativada:
+
`esxcfg-advcfg -g /Scsi/NVmeUseDsmTp4040`

+
O valor esperado é 1.



Para obter mais informações sobre o NVMe desalocar no VMware vSphere, consulte https://techdocs.broadcom.com/us/en/vmware-cis/vsphere/vsphere/8-0/vsphere-storage-8-0/storage-provisioning-and-space-reclamation-in-vsphere/storage-space-reclamation-in-vsphere.html["Recuperação de espaço de armazenamento no vSphere"^]



== Problemas conhecidos

A configuração de host NVMe-of para ESXi 8.x com ONTAP tem os seguintes problemas conhecidos:

[cols="10,30,30"]
|===
| ID de erro do NetApp | Título | Descrição 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | Nó ONTAP não operacional quando o protocolo NVMe/FC é usado com o ONTAP versão 9.9.1 | O ONTAP 9.9,1 introduziu o suporte para o comando NVMe "abort". Quando o ONTAP recebe o comando "abortar" para abortar um comando NVMe fundido que está aguardando o comando Partner, ocorre uma interrupção do nó ONTAP. O problema é notado somente em hosts que usam comandos fundidos NVMe (por exemplo, ESX) e transporte Fibre Channel (FC). 


| 1543660 | O erro de e/S ocorre quando as VMs Linux que usam adaptadores vNVMe encontram uma janela longa de todos os caminhos para baixo (APD)  a| 
As VMs Linux que executam o vSphere 8.x e posterior e que usam adaptadores NVMe virtuais (vNVME) encontram um erro de e/S porque a operação de repetição do vNVMe está desativada por padrão. Para evitar uma interrupção nas VMs Linux que executam kernels mais antigos durante um APD (All Paths Down) ou uma carga de e/S pesada, a VMware introduziu um "VSCSIDisableNvmeRetry" sintonizável para desativar a operação de repetição do vNVMe.

|===
.Informações relacionadas
link:https://docs.netapp.com/us-en/ontap-apps-dbs/vmware/vmware-vsphere-overview.html["VMware vSphere com ONTAP"^] link:https://kb.vmware.com/s/article/2031038["Suporte ao VMware vSphere 5.x, 6.x e 7.x com o NetApp MetroCluster (2031038)"^] link:https://kb.vmware.com/s/article/83370["Suporte ao VMware vSphere 6.x e 7.x com sincronização ativa do NetApp SnapMirror"^]
