---
sidebar: sidebar 
permalink: nvme_esxi_7.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: Descreve como configurar o NVMe-of para ESXi 7.x com o ONTAP 
---
= Configuração de host NVMe-of para ESXi 7.x com ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Você pode configurar o NVMe sobre Fabrics (NVMe-of) em hosts iniciadores que executam o ESXi 7.x e o ONTAP como destino.



== Capacidade de suporte

* A partir do ONTAP 9.7, o suporte a NVMe por canal de fibra (NVMe/FC) é adicionado às versões do VMware vSphere.
* A partir de 7.0U3c, o recurso NVMe/TCP é compatível com o hipervisor ESXi.
* A partir do ONTAP 9.10,1, o recurso NVMe/TCP é compatível com o ONTAP.




== Caraterísticas

* O host iniciador ESXi pode executar o tráfego NVMe/FC e FCP nas mesmas portas do adaptador. Consulte o link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] para obter uma lista de controladores e adaptadores FC compatíveis. Consulte a link:https://mysupport.netapp.com/matrix/["Ferramenta de Matriz de interoperabilidade"^] para obter a lista atual de configurações e versões suportadas.
* A partir do ONTAP 9.9,1 P3, o recurso NVMe/FC é compatível com a atualização 3 do ESXi 7,0.
* Para ESXi 7,0 e versões posteriores, o HPP (plugin de alto desempenho) é o plug-in padrão para dispositivos NVMe.




== Limitações conhecidas

As seguintes configurações não são suportadas:

* Mapeamento RDM
* Vols




== Habilite o NVMe/FC

. Verifique a string NQN do host ESXi e verifique se ela corresponde à string NQN do host para o subsistema correspondente na matriz ONTAP:
+
[listing]
----
# esxcli nvme  info get
Host NQN: nqn.2014-08.com.vmware:nvme:nvme-esx

# vserver nvme subsystem host show -vserver vserver_nvme
  Vserver Subsystem             Host NQN
  ------- ------------------- ----------------------------------------
  vserver_nvme ss_vserver_nvme nqn.2014-08.com.vmware:nvme:nvme-esx
----




=== Configurar Broadcom/Emulex

. Verifique se a configuração é suportada com o driver/firmware necessário consultando link:https://mysupport.netapp.com/matrix/["Ferramenta de Matriz de interoperabilidade"^]a .
. Defina o parâmetro do driver lpfc `lpfc_enable_fc4_type=3` para ativar o suporte a NVMe/FC no `lpfc` driver e reinicializar o host.



NOTE: A partir da atualização 3 do vSphere 7,0, o `brcmnvmefc` driver não está mais disponível. Portanto, o `lpfc` driver agora inclui o recurso NVMe sobre Fibre Channel (NVMe/FC) fornecido anteriormente com o `brcmnvmefc` driver.


NOTE: O `lpfc_enable_fc4_type=3` parâmetro é definido por padrão para os adaptadores da série LPe35000. Você deve executar o seguinte comando para defini-lo manualmente para adaptadores da série LPe32000 e da série LPe31000.

[listing]
----
# esxcli system module parameters set -m lpfc -p lpfc_enable_fc4_type=3

#esxcli system module parameters list  -m lpfc | grep lpfc_enable_fc4_type
lpfc_enable_fc4_type              int     3      Defines what FC4 types are supported

#esxcli storage core adapter list
HBA Name  Driver   Link State  UID                                   Capabilities         Description
--------  -------  ----------  ------------------------------------  -------------------  -----------
vmhba1    lpfc     link-up     fc.200000109b95456f:100000109b95456f  Second Level Lun ID  (0000:86:00.0) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter    FC HBA
vmhba2    lpfc     link-up     fc.200000109b954570:100000109b954570  Second Level Lun ID  (0000:86:00.1) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter    FC HBA
vmhba64   lpfc     link-up     fc.200000109b95456f:100000109b95456f                       (0000:86:00.0) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter   NVMe HBA
vmhba65   lpfc     link-up     fc.200000109b954570:100000109b954570                       (0000:86:00.1) Emulex Corporation Emulex LPe36000 Fibre Channel Adapter   NVMe HBA
----


=== Configure Marvell/QLogic

.Passos
. Verifique se a configuração é suportada com o driver/firmware necessário consultando link:https://mysupport.netapp.com/matrix/["Ferramenta de Matriz de interoperabilidade"^]a .
. Defina o `qlnativefc` parâmetro driver `ql2xnvmesupport=1` para ativar o suporte a NVMe/FC no `qlnativefc` driver e reinicie o host.
+
`# esxcfg-module -s 'ql2xnvmesupport=1' qlnativefc`

+

NOTE: O `qlnativefc` parâmetro driver é definido por padrão para os adaptadores QLE série 277x. Você deve executar o seguinte comando para configurá-lo manualmente para adaptadores da série QLE 277x.

+
[listing]
----
esxcfg-module -l | grep qlnativefc
qlnativefc               4    1912
----
. Verifique se o nvme está ativado no adaptador:
+
[listing]
----
  #esxcli storage core adapter list
HBA Name  Driver      Link State  UID                                   Capabilities         Description
--------  ----------  ----------  ------------------------------------  -------------------  -----------
 vmhba3    qlnativefc  link-up     fc.20000024ff1817ae:21000024ff1817ae  Second Level Lun ID  (0000:5e:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter    FC Adapter
vmhba4    qlnativefc  link-up     fc.20000024ff1817af:21000024ff1817af  Second Level Lun ID  (0000:5e:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter FC Adapter
vmhba64   qlnativefc  link-up     fc.20000024ff1817ae:21000024ff1817ae                       (0000:5e:00.0) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter  NVMe FC Adapter
vmhba65   qlnativefc  link-up     fc.20000024ff1817af:21000024ff1817af                       (0000:5e:00.1) QLogic Corp QLE2742 Dual Port 32Gb Fibre Channel to PCIe Adapter  NVMe FC Adapter
----




== Validar o NVMe/FC

. Verifique se o adaptador NVMe/FC está listado no host ESXi:
+
[listing]
----
# esxcli nvme adapter list

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:qlnativefc:21000024ff1817ae  FC              qlnativefc
vmhba65  aqn:qlnativefc:21000024ff1817af  FC              qlnativefc
vmhba66  aqn:lpfc:100000109b579d9c 	      FC              lpfc
vmhba67  aqn:lpfc:100000109b579d9d 	      FC              lpfc

----
. Verifique se os namespaces NVMe/FC são criados corretamente:
+
Os UUIDs no exemplo a seguir representam os dispositivos de namespace NVMe/FC.

+
[listing]
----
# esxcfg-mpath -b
uuid.5084e29a6bb24fbca5ba076eda8ecd7e : NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   vmhba65:C0:T0:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:69 WWPN: 21:00:34:80:0d:6d:72:69  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:2f:00:a0:98:df:e3:d1
   vmhba65:C0:T1:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:69 WWPN: 21:00:34:80:0d:6d:72:69  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:1a:00:a0:98:df:e3:d1
   vmhba64:C0:T0:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:68 WWPN: 21:00:34:80:0d:6d:72:68  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:18:00:a0:98:df:e3:d1
   vmhba64:C0:T1:L1 LUN:1 state:active fc Adapter: WWNN: 20:00:34:80:0d:6d:72:68 WWPN: 21:00:34:80:0d:6d:72:68  Target: WWNN: 20:17:00:a0:98:df:e3:d1 WWPN: 20:19:00:a0:98:df:e3:d1
----
+

NOTE: No ONTAP 9.7, o tamanho do bloco padrão para um namespace NVMe/FC é 4K. Este tamanho padrão não é compatível com ESXi. Portanto, ao criar namespaces para ESXi, você deve definir o tamanho do bloco de namespace como 512b. Você pode fazer isso usando o `vserver nvme namespace create` comando.

+
.Exemplo
`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

+
Consulte a link:https://docs.netapp.com/ontap-9/index.jsp?topic=%2Fcom.netapp.doc.dot-cm-cmpr%2FGUID-5CB10C70-AC11-41C0-8C16-B4D0DF916E9B.html["Páginas de manual do comando ONTAP 9"^]para obter mais detalhes.

. Verifique o status dos caminhos ANA individuais dos respetivos dispositivos de namespace NVMe/FC:
+
[listing]
----
esxcli storage hpp path list -d uuid.5084e29a6bb24fbca5ba076eda8ecd7e
fc.200034800d6d7268:210034800d6d7268-fc.201700a098dfe3d1:201800a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba64:C0:T0:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active
   Path Config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}

fc.200034800d6d7269:210034800d6d7269-fc.201700a098dfe3d1:201a00a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba65:C0:T1:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active
   Path Config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}

fc.200034800d6d7269:210034800d6d7269-fc.201700a098dfe3d1:202f00a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba65:C0:T0:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active unoptimized
   Path Config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}

fc.200034800d6d7268:210034800d6d7268-fc.201700a098dfe3d1:201900a098dfe3d1-uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Runtime Name: vmhba64:C0:T1:L1
   Device: uuid.5084e29a6bb24fbca5ba076eda8ecd7e
   Device Display Name: NVMe Fibre Channel Disk (uuid.5084e29a6bb24fbca5ba076eda8ecd7e)
   Path State: active unoptimized
   Path Config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}
----




== Configurar o NVMe/TCP

A partir de 7.0U3c, os módulos NVMe/TCP necessários serão carregados por padrão. Para configurar a rede e o adaptador NVMe/TCP, consulte a documentação do VMware vSphere.



== Valide o NVMe/TCP

.Passos
. Verifique o status do adaptador NVMe/TCP.
+
[listing]
----
[root@R650-8-45:~] esxcli nvme adapter list
Adapter    Adapter Qualified Name
--------- -------------------------------
vmhba64    aqn:nvmetcp:34-80-0d-30-ca-e0-T
vmhba65    aqn:nvmetc:34-80-13d-30-ca-e1-T
list
Transport Type   Driver   Associated Devices
---------------  -------  ------------------
TCP              nvmetcp    vmnzc2
TCP              nvmetcp    vmnzc3
----
. Para listar as conexões NVMe/TCP, use o seguinte comando:
+
[listing]
----
[root@R650-8-45:~] esxcli nvme controller list
Name
-----------
nqn.1992-08.com.netapp:sn.5e347cf68e0511ec9ec2d039ea13e6ed:subsystem.vs_name_tcp_ss#vmhba64#192.168.100.11:4420
nqn.1992-08.com.netapp:sn.5e347cf68e0511ec9ec2d039ea13e6ed:subsystem.vs_name_tcp_ss#vmhba64#192.168.101.11:4420
Controller Number  Adapter   Transport Type   IS Online
----------------- ---------  ---------------  ---------
1580              vmhba64    TCP              true
1588              vmhba65    TCP              true

----
. Para listar o número de caminhos para um namespace NVMe, use o seguinte comando:
+
[listing]
----
[root@R650-8-45:~] esxcli storage hpp path list -d uuid.400bf333abf74ab8b96dc18ffadc3f99
tcp.vmnic2:34:80:Od:30:ca:eo-tcp.unknown-uuid.400bf333abf74ab8b96dc18ffadc3f99
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.400bf333abf74ab8b96dc18ffadc3f99
   Device Display Name: NVMe TCP Disk (uuid.400bf333abf74ab8b96dc18ffadc3f99)
   Path State: active unoptimized
   Path config: {TPG_id=0,TPG_state=ANO,RTP_id=0,health=UP}

tcp.vmnic3:34:80:Od:30:ca:el-tcp.unknown-uuid.400bf333abf74ab8b96dc18ffadc3f99
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.400bf333abf74ab8b96dc18ffadc3f99
   Device Display Name: NVMe TCP Disk (uuid.400bf333abf74ab8b96dc18ffadc3f99)
   Path State: active
   Path config: {TPG_id=0,TPG_state=AO,RTP_id=0,health=UP}
----




== Problemas conhecidos

A configuração de host NVMe-of para ESXi 7.x com ONTAP tem os seguintes problemas conhecidos:

[cols="10,30,30"]
|===
| ID de erro do NetApp | Título | Solução alternativa 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | Nó ONTAP não operacional quando o protocolo NVMe/FC é usado com o ONTAP versão 9.9.1 | Verifique e retifique quaisquer problemas de rede na estrutura do host. Se isso não ajudar, atualize para um patch que corrija esse problema. 
|===
.Informações relacionadas
link:https://docs.netapp.com/us-en/ontap-apps-dbs/vmware/vmware-vsphere-overview.html["VMware vSphere com ONTAP"^] link:https://kb.vmware.com/s/article/2031038["Suporte ao VMware vSphere 5.x, 6.x e 7.x com o NetApp MetroCluster (2031038)"^] link:https://kb.vmware.com/s/article/83370["Suporte ao VMware vSphere 6.x e 7.x com o NetApp SnapMirror ative Sync"^]
